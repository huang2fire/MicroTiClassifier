{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tomllib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir():\n",
    "    output_dir = Path(\"../output\")\n",
    "    dir_dict = {\n",
    "        \"chart\": output_dir / \"chart\",\n",
    "        \"checkpoints\": output_dir / \"checkpoints\",\n",
    "        \"log\": output_dir / \"log\",\n",
    "    }\n",
    "    for dir_path in dir_dict.values():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c817c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path: Path, val_path: Path) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_transform = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "            v2.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.3),\n",
    "            v2.RandomEqualize(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            v2.Resize(256),\n",
    "            v2.CenterCrop(224),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_path, transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset, batch_size=32, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epoch_time(start_time: float, end_time: float) -> Tuple[int, int]:\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd516ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch_info(\n",
    "    epoch: int, epoch_mins: int, epoch_secs: int, train_log: dict, val_log: dict\n",
    ") -> None:\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(\n",
    "        f\"Train Loss: {train_log['loss']:.4f} | Train Acc: {train_log['accuracy']:.4f}\"\n",
    "    )\n",
    "    print(f\"Val Loss: {val_log['loss']:.4f} | Val Acc: {val_log['accuracy']:.4f}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    iterator: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: str,\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    preds_list, labels_list = [], []\n",
    "\n",
    "    for images, labels in tqdm(iterator, desc=\"Training\", unit=\"batch\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(input=outputs, dim=1)\n",
    "        preds_list.extend(preds.cpu().numpy())\n",
    "        labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        \"loss\": epoch_loss / len(iterator),\n",
    "        \"accuracy\": accuracy_score(labels_list, preds_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c48518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(\n",
    "    model: nn.Module, iterator: DataLoader, criterion: nn.Module, device: str\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    preds_list, labels_list = [], []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in tqdm(iterator, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(input=outputs, dim=1)\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        \"loss\": epoch_loss / len(iterator),\n",
    "        \"accuracy\": accuracy_score(labels_list, preds_list),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d770ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    lr_scheduler: optim.lr_scheduler.LRScheduler,\n",
    "    criterion: nn.Module,\n",
    "    device: str,\n",
    "    N_EPOCHS: int,\n",
    ") -> None:\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    best_dict = {\"val_accuracy\": 0, \"train_accuracy\": 0, \"model_path\": None}\n",
    "    log_list = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_log_dict = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_log_dict = evaluate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = calculate_epoch_time(start_time, end_time)\n",
    "        print_epoch_info(epoch, epoch_mins, epoch_secs, train_log_dict, val_log_dict)\n",
    "\n",
    "        update_flag = val_log_dict[\"accuracy\"] > best_dict[\"val_accuracy\"] or (\n",
    "            val_log_dict[\"accuracy\"] == best_dict[\"val_accuracy\"]\n",
    "            and train_log_dict[\"accuracy\"] > best_dict[\"train_accuracy\"]\n",
    "        )\n",
    "\n",
    "        if update_flag:\n",
    "            if best_dict[\"model_path\"] is not None:\n",
    "                os.remove(best_dict[\"model_path\"])\n",
    "\n",
    "            best_dict.update(\n",
    "                {\n",
    "                    \"val_accuracy\": val_log_dict[\"accuracy\"],\n",
    "                    \"train_accuracy\": train_log_dict[\"accuracy\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            best_dict[\"model_path\"] = (\n",
    "                f\"../output/checkpoints/Ti5_{timestamp}_{best_dict['val_accuracy']:.3f}.pt\"\n",
    "            )\n",
    "\n",
    "            torch.save(model, best_dict[\"model_path\"])\n",
    "            print(\"已保存新的最佳模型!\")\n",
    "\n",
    "        log_list.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"epoch_mins\": epoch_mins,\n",
    "                \"epoch_secs\": epoch_secs,\n",
    "                \"train_loss\": train_log_dict[\"loss\"],\n",
    "                \"train_accuracy\": train_log_dict[\"accuracy\"],\n",
    "                \"val_loss\": val_log_dict[\"loss\"],\n",
    "                \"val_accuracy\": val_log_dict[\"accuracy\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    pd.DataFrame(log_list).to_csv(\n",
    "        f\"../output/log/train_log_{timestamp}.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d29ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_strategy(\n",
    "    strategy: str, num_classes: int, parameter: Dict[str, int | float]\n",
    ") -> Tuple[nn.Module, optim.Optimizer]:\n",
    "    model = models.vgg16(\n",
    "        weights=models.VGG16_Weights.DEFAULT if (strategy != \"zero\") else None\n",
    "    )\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "\n",
    "    # 策略 2: 冻结特征层 - 只训练分类层\n",
    "    if strategy == \"frozen_features\":\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        optimizer = optim.AdamW(\n",
    "            params=model.classifier.parameters(),\n",
    "            lr=parameter[\"base_lr\"],\n",
    "            weight_decay=parameter[\"weight_decay\"],\n",
    "        )\n",
    "    # 策略3: 全部微调 - 所有层相同学习率 and 策略 1: 不使用迁移学习\n",
    "    elif strategy == \"full\" or strategy == \"zero\":\n",
    "        optimizer = optim.AdamW(\n",
    "            params=model.parameters(),\n",
    "            lr=parameter[\"base_lr\"],\n",
    "            weight_decay=parameter[\"weight_decay\"],\n",
    "        )\n",
    "    # 策略 4: 全部微调 - 分层学习率\n",
    "    elif strategy == \"full_layerwise\":\n",
    "        optimizer = optim.AdamW(\n",
    "            params=[\n",
    "                {\n",
    "                    \"params\": model.features.parameters(),\n",
    "                    \"lr\": parameter[\"features_lr\"],\n",
    "                },\n",
    "                {\n",
    "                    \"params\": model.classifier.parameters(),\n",
    "                    \"lr\": parameter[\"classifier_lr\"],\n",
    "                },\n",
    "            ],\n",
    "            weight_decay=parameter[\"weight_decay\"],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"未知策略: {strategy}\")\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with open(\"../config/train.toml\", \"rb\") as file:\n",
    "        parameter = tomllib.load(file)\n",
    "\n",
    "    create_dir()\n",
    "\n",
    "    dataset_path = Path(\"../data/Ti5_split\")\n",
    "    train_path, val_path = dataset_path / \"train\", dataset_path / \"val\"\n",
    "    train_loader, val_loader = load_data(train_path, val_path)\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model, optimizer = initial_strategy(\n",
    "        strategy=\"zero\",\n",
    "        num_classes=len(train_loader.dataset.classes),\n",
    "        parameter=parameter,\n",
    "    )\n",
    "\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=parameter[\"epoch\"] / 10,\n",
    "        num_training_steps=parameter[\"epoch\"],\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        criterion,\n",
    "        DEVICE,\n",
    "        parameter[\"epoch\"],\n",
    "    )\n",
    "\n",
    "    print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MicroTiClassifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
